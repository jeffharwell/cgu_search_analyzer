#!/usr/bin/python2.7

import csv
import urllib2
import urllib
import simplejson
import sys
import time
import os
import re
import random
from HTMLParser import HTMLParser

"""
This code reads the CSV file generated by 'filter_sample_file.py' or
even, if you know what you are doing, 'write_to_file_[23]gm.py'. It will
contact Google and do a search using each query plus the expansion term and 
then write the JSON file returned by Google into a ./results directory.

It is relatively smart about continuing from the point where it is interrupted.
It can also do a random back-off when Google sends a 'quota exceeded' message.
The delays in the code are designed to keep the process under the quote limit
for the Google search "api".

Usage: get_google_results.py file_with_expansions {s,c} offset

The "offset" is a number that the program will add to the query number when
creating the filename used to save the JSON. This allows you to run multiple copies
or run across multiple servers and not have to re-name all of the files when you
then merge the results.
"""

class MLStripper(HTMLParser):
    def __init__(self):
        self.reset()
        self.fed = []
    def handle_data(self, d):
        self.fed.append(d)
    def get_data(self):
        return ''.join(self.fed)

def strip_tags(html):
    s = MLStripper()
    s.feed(html)
    return s.get_data()

def is_integer(var):
    try:
        int(var)
        return True
    except ValueError:
        return False


def find_continue_point(base_dir, expansion_type, offset):
    def is_valid_file(var):
        if re.match(r'^%s_[0-9]+' % expansion_type, var):
            return True
        else:
            return False

    c = os.listdir(base_dir)
    fn_list = filter(is_valid_file, c)
    if len(fn_list) == 0:
        return 0

    str_num_list = [x.split('_')[1] for x in c]
    num_list = [int(x) - offset for x in str_num_list] 
    return max(num_list)

def google_search(terms, file_to_write):

    query_string = "%s %s" % (terms[0],terms[1])
    query = urllib.urlencode({'q':query_string})
    url = ('https://ajax.googleapis.com/ajax/services/search/web'
   '?v=1.0&rsz=8&%s' % query)
    print url


    while 1:

        request = urllib2.Request(
                  url, None, {'Referer':'cisat.claremont.edu'})
        response = urllib2.urlopen(request)
        results = simplejson.load(response)

        if results['responseDetails'] == "Quota Exceeded.  Please see http://code.google.com/apis/websearch":
            print "Hit our quote, sleeping for a while"
            r = random.sample([60,120,180,240,300],1)
            time.sleep(300 + r[0])
        else:
            f = open(to_write, 'w')
            f.write(simplejson.dumps(results))
            f.close()
            break

base_dir = "results"
query_number = 0
file_to_open = sys.argv[1]
expansion_type = sys.argv[2].strip()
offset = int(sys.argv[3].strip())
if expansion_type not in ['s','c']:
    raise RuntimeError, "Invalid expansion type %s" % expansion_type
if not file_to_open or file_to_open == "":
    raise RuntimeError, "No File passed to open %s" % file_to_open
if not offset >= 0:
    raise RuntimeError, "%s is not a valid offset" % offset

continue_point = find_continue_point(base_dir, expansion_type, offset)
print "Continuing from %s" % continue_point

do_only = []

with open(file_to_open, 'rb') as csvfile:
    reader = csv.reader(csvfile, delimiter=',', quotechar='"')
    for row in reader:
        query_number += 1
        if query_number <= continue_point:
            continue
        if len(do_only) > 0 and not query_number in do_only:
            continue

        to_write = "./%s/%s_%s" % (base_dir, expansion_type, query_number+offset)
        google_search([row[5],row[0]],to_write)

        r = random.sample([5,10,15,20],1)
        time.sleep(25+r[0])

print "Done processing %s query expansions" % expansion_type
sys.exit()

